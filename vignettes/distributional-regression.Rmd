---
title: "Distributional Regression for Motorcycle Crash Data with Time-Varying Location and Scale Parameters"
output: 
  rmarkdown::html_vignette:
    toc: true
    toc_depth: 3
vignette: >
  %\VignetteIndexEntry{Distributional Regression for Motorcycle Crash Data with Time-Varying Location and Scale Parameters}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 7,
  fig.height = 5,
  warning = FALSE,
  message = FALSE
)
```

```{r setup}
library(ffc)
library(mgcv)
library(MASS)
library(dplyr)
library(ggplot2)
theme_set(theme_bw())
```

## Introduction

Motorcycle crash dynamics involve complex patterns where both the mean acceleration and its variability change dramatically over time during impact events. Traditional Gaussian regression assumes constant variance, potentially missing critical safety insights about periods of extreme or unpredictable forces. This vignette demonstrates how distributional regression with the [`ffc`](https://github.com/njtierney/ffc) package addresses these challenges through:

1. Time-varying location parameters capturing evolving mean acceleration patterns  
2. Time-varying scale parameters modeling heteroskedasticity during impact phases    
3. Rolling forecast evaluation for robust model validation  
4. Comprehensive uncertainty quantification for safety applications  

### The crash dynamics challenge

Motorcycle crash acceleration data presents unique modeling challenges:

- Non-constant variance during different impact phases  
- Complex temporal evolution of both mean and variability  
- Need to predict both typical and extreme force scenarios  
- Critical safety implications requiring proper uncertainty bounds

### Our approach: distributional GAMs with functional coefficients

The `ffc` package treats both location and scale as smooth functions that evolve over time. By combining [`mgcv`](https://cran.r-project.org/package=mgcv) distributional families with functional regression, we can:

- Model time-varying mean acceleration trends  
- Capture evolving variance patterns throughout crash sequences    
- Generate probabilistic forecasts with parameter-specific uncertainty  
- Validate predictions using rolling forecast techniques  

## Data exploration

We'll use the classic motorcycle crash dataset from [`MASS`](https://cran.r-project.org/package=MASS), representing head acceleration measurements during simulated motorcycle crashes:

```{r data-overview}
data(mcycle, package = "MASS")
mcycle$index <- 1:NROW(mcycle)

# Check data structure
glimpse(mcycle)
```

### Visualizing acceleration patterns

```{r acceleration-patterns, fig.cap="Head acceleration measurements showing clear heteroskedasticity over time."}
ggplot(mcycle, aes(x = index, y = accel)) +
  geom_point(alpha = 0.7, size = 2) +
  geom_line() +
  geom_vline(xintercept = 110, linetype = "dashed", color = "darkred", alpha = 0.7) +
  labs(
    x = "Time Index", 
    y = "Head Acceleration",
    title = "Motorcycle crash data: heteroskedastic acceleration patterns"
  )
```

### Identifying variance patterns

Let's examine how variance changes across different phases of the crash sequence:

```{r variance-exploration, fig.cap="Moving variance reveals dramatic changes in acceleration variability over time."}
# Calculate moving variance with overlapping windows
window_size <- 15
mcycle$moving_var <- NA

for (i in (window_size + 1):(nrow(mcycle) - window_size)) {
  window_data <- mcycle$accel[(i - window_size):(i + window_size)]
  mcycle$moving_var[i] <- var(window_data, na.rm = TRUE)
}

ggplot(mcycle, aes(x = index)) +
  geom_line(aes(y = moving_var), color = "darkred", linewidth = 1.2) +
  geom_point(aes(y = moving_var), color = "darkred", alpha = 0.7) +
  labs(
    x = "Time Index",
    y = "Moving variance",
    title = "Time-varying variance in motorcycle crash acceleration"
  )
```

## Modeling with distributional GAMs

### Train-test split for validation

We'll use early crash data for training and reserve later impact phases for validation:

```{r data-split}
mcycle_train <- mcycle |>
  dplyr::filter(index < 110)
mcycle_test <- mcycle |>
  dplyr::filter(index >= 110)
```

### The distributional model specification

Now let's fit a model that will allow us to forecast changes in both the mean *and* the variance. The key innovation is using [`gaulss()`](https://rdrr.io/cran/mgcv/man/gaulss.html) to model both location and scale parameters with functional time series:

```{r fit-model}
mcycle_gaulss_model <- ffc_gam(
  list(
    # Location parameter: time-varying mean acceleration  
    accel ~ fts(index, mean_only = TRUE, time_k = 25),
    
    # Scale parameter: time-varying variance
    ~ fts(index, mean_only = TRUE, time_k = 25)
  ),
  family = gaulss(),
  data = mcycle_train,
  time = "index"
)
```

### Understanding the model structure

```{r model-summary}
summary(mcycle_gaulss_model)
```

The model decomposes crash dynamics into:  

1. **Location parameter**: Smooth time-varying mean acceleration    
2. **Scale parameter**: Time-varying standard deviation modeling heteroskedasticity    

The [`gaulss()`](https://rdrr.io/cran/mgcv/man/gaulss.html) family uses log-link for the scale parameter, ensuring positive variance estimates throughout the crash sequence.

## Model diagnostics and interpretation

### Visualizing fitted trends and variance

We can examine how the model captures both mean trends and changing variance:

```{r fitted-patterns, fig.cap="Fitted location and scale parameters reveal distinct temporal patterns in crash dynamics."}
# Extract time-varying coefficients for both parameters
fts_results <- fts_coefs(mcycle_gaulss_model, summary = FALSE)

autoplot(fts_results) +
  labs(
    title = "Time-varying distributional parameters",
    x = "Time Index",
    y = "Coefficient Value"
  )
```

### Extracting parameter-specific coefficients

The functional coefficients reveal the underlying dynamics of each distributional parameter:

```{r extract-coefficients}
# View coefficient structure
fts_results

# Summary statistics by parameter
fts_results |>
  as_tibble() |>
  group_by(.parameter, .basis) |>
  summarise(
    mean_coef = mean(.estimate),
    sd_coef = sd(.estimate),
    .groups = "drop"
  )
```

## Forecasting distributional models

### Single forecast demonstration

Generate forecasts that provide both mean predictions and uncertainty intervals based on the evolving variance structure:

```{r single-forecast}
fc_single <- forecast(
  mcycle_gaulss_model,
  newdata = mcycle_test,
  model = "ENS",
  mean_model = "ENS"
)

# Examine forecast structure
fc_single
```

### Visualizing distributional forecasts

```{r forecast-visualization, fig.cap="Distributional forecasts capture both mean trends and evolving uncertainty from the time-varying scale parameter."}
# Combine training and forecast data for visualization
forecast_plot_data <- mcycle_train |>
  as_tibble() |>
  mutate(type = "observed") |>
  bind_rows(
    mcycle_test |>
      as_tibble() |>
      bind_cols(fc_single) |>
      mutate(type = "forecast")
  )

ggplot(forecast_plot_data, aes(x = index, y = accel)) +
  geom_point(data = filter(forecast_plot_data, type == "observed"),
             alpha = 0.7, size = 2) +
  geom_ribbon(data = filter(forecast_plot_data, type == "forecast"),
              aes(ymin = .q2.5, ymax = .q97.5),
              fill = "darkred", alpha = 0.2) +
  geom_ribbon(data = filter(forecast_plot_data, type == "forecast"),
              aes(ymin = .q10, ymax = .q90),
              fill = "darkred", alpha = 0.3) +
  geom_line(data = filter(forecast_plot_data, type == "forecast"),
            aes(y = .estimate),
            colour = "darkred", linewidth = 1.2) +
  geom_point(data = filter(forecast_plot_data, type == "forecast"),
             color = "black", size = 2) +
  labs(
    title = "Distributional GAM forecasts for motorcycle crash acceleration",
    x = "Time Index", 
    y = "Head Acceleration"
  )
```

## Coefficient forecast inspection

We can also forecast the functional coefficients themselves to understand how the underlying parameters evolve:

```{r coefficient-forecasting, fig.cap="Forecasted functional coefficients show parameter-specific evolution patterns with quantified uncertainty."}
# Forecast the coefficients to the test period
coef_forecast_raw <- forecast(
  fts_results,
  h = nrow(mcycle_test),
  model = "ENS"
)

# Summarize coefficient forecasts
coef_forecast <- coef_forecast_raw |>
  as_tibble() |>
  group_by(.basis, .time) |>
  summarise(
    .estimate = mean(.sim),
    .q2.5 = quantile(.sim, 0.025),
    .q97.5 = quantile(.sim, 0.975),
    .groups = "drop"
  )

# Summarize training coefficients
fts_coefs_summary <- fts_results |>
  as_tibble() |>
  group_by(.basis, .time) |>
  summarise(
    .estimate = mean(.estimate),
    .groups = "drop"
  )

# Visualize coefficient evolution and forecasts
ggplot() +
  geom_line(data = fts_coefs_summary,
            aes(x = .time, y = .estimate, color = .basis),
            linewidth = 1) +
  geom_ribbon(data = coef_forecast,
              aes(x = .time, ymin = .q2.5, ymax = .q97.5, fill = .basis),
              alpha = 0.2) +
  geom_line(data = coef_forecast,
            aes(x = .time, y = .estimate, color = .basis),
            linewidth = 1.2) +
  facet_wrap(~ .basis, scales = "free_y") +
  labs(
    title = "Functional coefficients: training vs forecasted",
    x = "Time Index", 
    y = "Coefficient Value"
  ) +
  theme(legend.position = "none")
```

## Rolling forecast evaluation

Rolling forecast evaluation provides robust validation for distributional models by testing performance across multiple forecast origins:

```{r rolling-setup}
# Rolling forecast parameters
window_size <- 80
forecast_horizon <- 5
step_size <- 5

# Calculate forecast origins
max_start <- nrow(mcycle) - forecast_horizon
forecast_starts <- seq(1, max(1, max_start - window_size), by = step_size)
```

### Executing rolling forecasts

```{r rolling-forecasts}
# Storage for results
rolling_results <- list()

# Perform rolling forecasts
for (i in seq_along(forecast_starts)) {
  start_idx <- forecast_starts[i]
  end_idx <- min(start_idx + window_size - 1, nrow(mcycle))

  # Training data for this iteration
  train_data <- mcycle[start_idx:end_idx, ]
  
  # Test data
  test_start <- end_idx + 1
  test_end <- min(test_start + forecast_horizon - 1, nrow(mcycle))

  if (test_end <= nrow(mcycle)) {
    test_data <- mcycle[test_start:test_end, ]

    # Fit model with reduced complexity for rolling validation
    roll_model <- ffc_gam(
      list(
        accel ~ fts(index, mean_only = TRUE, time_k = 10),
        ~ fts(index, mean_only = TRUE, time_k = 10)
      ),
      family = gaulss(),
      data = train_data,
      time = "index"
    )

    # Generate forecasts
    roll_forecast <- forecast(
      roll_model,
      newdata = test_data,
      model = "ENS"
    )

    # Store results with metadata
    rolling_results[[i]] <- test_data |>
      bind_cols(roll_forecast) |>
      mutate(
        forecast_origin = i,
        train_start = start_idx,
        train_end = end_idx,
        horizon = 1:nrow(test_data)
      )
  }
}

# Combine all rolling forecast results
rolling_forecasts <- bind_rows(rolling_results)
```

## Rolling forecast performance analysis

### Forecast accuracy metrics

```{r performance-analysis}
# Calculate forecast errors and coverage statistics
rolling_forecasts <- rolling_forecasts |>
  mutate(
    forecast_error = accel - .estimate,
    abs_error = abs(forecast_error),
    squared_error = forecast_error^2,
    coverage_95 = (accel >= .q2.5) & (accel <= .q97.5),
    coverage_80 = (accel >= .q10) & (accel <= .q90),
    interval_width_95 = .q97.5 - .q2.5,
    interval_width_80 = .q90 - .q10
  )

# Performance by forecast horizon
horizon_stats <- rolling_forecasts |>
  group_by(horizon) |>
  summarise(
    n_forecasts = n(),
    mae = mean(abs_error, na.rm = TRUE),
    rmse = sqrt(mean(squared_error, na.rm = TRUE)),
    coverage_95 = mean(coverage_95, na.rm = TRUE),
    coverage_80 = mean(coverage_80, na.rm = TRUE),
    avg_width_95 = mean(interval_width_95, na.rm = TRUE),
    avg_width_80 = mean(interval_width_80, na.rm = TRUE),
    .groups = 'drop'
  )

print(horizon_stats)

# Overall performance summary  
overall_stats <- rolling_forecasts |>
  summarise(
    total_forecasts = n(),
    overall_mae = mean(abs_error, na.rm = TRUE),
    overall_rmse = sqrt(mean(squared_error, na.rm = TRUE)),
    overall_coverage_95 = mean(coverage_95, na.rm = TRUE),
    overall_coverage_80 = mean(coverage_80, na.rm = TRUE)
  )

cat("- 95% coverage:", round(overall_stats$overall_coverage_95 * 100, 1), "%\n")
cat("- 80% coverage:", round(overall_stats$overall_coverage_80 * 100, 1), "%\n")
```

### Rolling forecast visualization

```{r rolling-visualization, fig.cap="Rolling forecast evaluation reveals consistent prediction performance across different crash phases."}
# Visualize rolling forecasts over the full time series
ggplot() +
  geom_point(data = mcycle, aes(x = index, y = accel),
             alpha = 0.4, size = 1) +
  geom_point(data = rolling_forecasts,
             aes(x = index, y = .estimate),
             color = "darkred", size = 1.5, alpha = 0.8) +
  geom_errorbar(data = rolling_forecasts,
                aes(x = index, ymin = .q10, ymax = .q90),
                color = "darkred", alpha = 0.6, width = 0.5) +
  geom_segment(data = rolling_forecasts,
               aes(x = index, xend = index, y = accel, yend = .estimate),
               color = "blue", alpha = 0.3, linetype = "dashed") +
  labs(
    title = "Rolling forecast evaluation across crash timeline",
    x = "Time Index", 
    y = "Head Acceleration"
  )
```

### Coverage evolution analysis

```{r coverage-evolution, fig.cap="Cumulative coverage rates demonstrate well-calibrated prediction intervals throughout the evaluation."}
# Track coverage evolution over evaluation period
coverage_data <- rolling_forecasts |>
  arrange(index) |>
  mutate(
    running_coverage_95 = cumsum(coverage_95) / row_number(),
    running_coverage_80 = cumsum(coverage_80) / row_number()
  )

ggplot(coverage_data, aes(x = index)) +
  geom_line(aes(y = running_coverage_95), color = "darkred", linewidth = 1.2) +
  geom_line(aes(y = running_coverage_80), color = "blue", linewidth = 1.2) +
  geom_hline(yintercept = 0.95, linetype = "dashed", color = "darkred", alpha = 0.7) +
  geom_hline(yintercept = 0.80, linetype = "dashed", color = "blue", alpha = 0.7) +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(
    title = "Cumulative coverage rates validate interval calibration",
    x = "Time Index",
    y = "Cumulative Coverage Rate"
  )
```

## Key insights and best practices

### When to use distributional regression

Distributional regression with `ffc` is essential when:  

- Data exhibits clear heteroskedasticity (changing variance over time)  
- Both mean trends and uncertainty evolution matter for interpretation  
- Risk assessment requires proper uncertainty bounds  
- Traditional constant-variance assumptions are violated  

Key implementation considerations:

- Use list formula syntax: `list(response ~ fts(...), ~ fts(...))`    
- Choose appropriate distributional families   ([`gaulss()`](https://rdrr.io/cran/mgcv/man/gaulss.html), [`twlss()`](https://rdrr.io/cran/mgcv/man/twlss.html), [`betar()`](https://rdrr.io/cran/mgcv/man/betar.html))  
- Scale parameter automatically uses log-link for positivity  
- Different `time_k` values can be used for different parameters  

### Distributional forecasting advantages

The time-varying distributional approach excels when:  

- Traditional homoskedastic models miss variance patterns  
- Parameter-specific uncertainty quantification is needed    
- Rolling validation reveals model robustness across conditions  
- Safety applications require conservative interval estimation  

For detailed guidance on distributional families in [`mgcv`](https://cran.r-project.org/package=mgcv), see [Wood (2017)](https://www.routledge.com/Generalized-Additive-Models-An-Introduction-with-R-Second-Edition/Wood/p/book/9781498728331).

## Conclusion

This vignette demonstrated advanced distributional regression techniques for motorcycle crash data:
  
- Time-varying location and scale parameters capture evolving crash dynamics  
- Functional time series coefficients provide interpretable parameter evolution  
- Rolling forecast evaluation ensures robust model validation  
- Proper uncertainty quantification enables safety-critical applications  

The `ffc` package integrates distributional modeling seamlessly with functional forecasting:
- List formula syntax for multi-parameter specifications  
- Parameter-specific coefficient extraction and forecasting    
- Comprehensive validation through rolling forecast techniques  
- Integration with [`mgcv`](https://cran.r-project.org/package=mgcv) distributional families

### Further reading

- Wood (2017): [Generalized Additive Models: An Introduction with R](https://www.routledge.com/Generalized-Additive-Models-An-Introduction-with-R-Second-Edition/Wood/p/book/9781498728331)  
- Rigby & Stasinopoulos (2005): [Generalized additive models for location, scale and shape](https://doi.org/10.1111/j.1467-9876.2005.00510.x)    
- Hyndman & Athanasopoulos (2021): [Forecasting: Principles and Practice](https://otexts.com/fpp3/)  
- [mgcv distributional families documentation](https://rdrr.io/cran/mgcv/man/family.mgcv.html)
